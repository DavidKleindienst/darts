"""
Shap-based ForecastingModelExplainer
------------------------------
This class is meant to wrap a shap explainer (https://github.com/slundberg/shap) specifically for time series.

Warning

This is only a shap value of direct influence and doesn't take into account relationships
between past lags themselves. Hence a given past lag could also have an indirect influence via the
intermediate past lags elements between it and the time step we want to explain, if we assume that
the intermediate past lags are generated by the same model.

TODO
    - Optional De-trend  if the timeseries is not stationary.
    There would be 1) a stationarity test and 2) a de-trend methodology for the target. It can be for
    example target - moving_average(input_chunk_length).

"""

from cmath import exp
from enum import Enum
from hashlib import new
from os import sched_get_priority_max
from typing import Optional, Sequence, Union, Dict, NewType

import matplotlib.pyplot as plt
from numpy import integer
import pandas as pd
import shap
from sklearn.multioutput import MultiOutputRegressor

from darts import TimeSeries
from darts.explainability.explainability import ForecastingModelExplainer
from darts.logging import get_logger, raise_if, raise_log
from darts.models.forecasting.forecasting_model import (
    ForecastingModel,
    GlobalForecastingModel,
)
from darts.models.forecasting.regression_model import RegressionModel
from darts.utils import retain_period_common_to_all

logger = get_logger(__name__)


class _ShapMethod(Enum):
    TREE = 0
    GRADIENT = 1
    DEEP = 2
    KERNEL = 3
    SAMPLING = 4
    PARTITION = 5
    LINEAR = 6
    PERMUTATION = 7
    ADDITIVE = 8


ShapMethod = NewType("ShapMethod", _ShapMethod)

default_sklearn_shap_explainers = {
    "AdaBoostRegressor": _ShapMethod.PERMUTATION,
    "BaggingRegressor": _ShapMethod.PERMUTATION,
    "ExtraTreesRegressor": _ShapMethod.TREE,
    "GradientBoostingRegressor": _ShapMethod.TREE,
    "LGBMRegressor": _ShapMethod.TREE,
    "RandomForestRegressor": _ShapMethod.TREE,
    "LinearRegression": _ShapMethod.LINEAR,
}


class ShapExplainer(ForecastingModelExplainer):
    def __init__(
        self,
        model: ForecastingModel,
        background_series: Optional[Union[TimeSeries, Sequence[TimeSeries]]] = None,
        background_past_covariates: Optional[
            Union[TimeSeries, Sequence[TimeSeries]]
        ] = None,
        background_future_covariates: Optional[
            Union[TimeSeries, Sequence[TimeSeries]]
        ] = None,
        shap_method: Optional[str] = None,
        **kwargs,
    ):
        """ShapExplainer

        Nomenclature:
        - A background time series is a time series with which we train the Explainer model.
        - A foreground time series is the time series we will explain according to the fitted Explainer model.

        Parameters
        ----------
        model
            A ForecastingModel we want to explain. It has to be fitted first. Only RegressionModel type for now.
        background_series
            Optionally, a TimeSeries or a list of time series we want to use to compare with any foreground we want to explain.
            This is optional, for 2 reasons:
                - In general we want to keep the training_series of the model and this is the default one,
                but in case of multiple time series training (global or meta learning) the ForecastingModel doesn't
                save them. In this case we need to feed a background time series.
                - We might want to consider a reduced well chosen background in order to reduce computation
                time.
        background_past_covariates
            Optionally, a past covariates TimeSeries or list of TimeSeries that the model needs once fitted.
        background_future_covariates
            Optionally, a future covariates TimeSeries or list of TimeSeries that the model needs once fitted.
        shap_method
            Optionally, a shap method we want to apply. By default, the method is chosen automatically with an internal mapping.
            Supported values : “permutation”, “partition”, “tree”, “kernel”, “sampling”, “linear”, “deep”, “gradient”
        **kwargs
            Optionally, an additional keyword arguments passed to the shap_method chosen, if any.
        """
        if not issubclass(type(model), RegressionModel):
            raise_log(
                ValueError(
                    "Invalid model type. For now, only RegressionModel type can be explained."
                ),
                logger,
            )

        super().__init__(
            model,
            background_series,
            background_past_covariates,
            background_future_covariates,
        )

        # As we only use RegressionModel, we fix the forecast n step ahead we want to explain as
        # output_chunk_length
        self.n = self.model.output_chunk_length

        if shap_method is not None:
            if shap_method.upper() in _ShapMethod.__members__:
                self.shap_method = _ShapMethod[shap_method.upper()]
            else:
                raise_log(
                    ValueError(
                        "Invalid shap method. Please choose one value among the following: [partition, tree, kernel, sampling, linear, deep, gradient]."
                    )
                )
        else:
            self.shap_method = None

        self.explainers = _RegressionShapExplainers(
            self.model,
            self.background_series,
            self.background_past_covariates,
            self.background_future_covariates,
            n=self.n,
            shap_method=self.shap_method,
            **kwargs,
        )

    def explain(
        self,
        foreground_series: Optional[TimeSeries],
        foreground_past_covariates: Optional[TimeSeries],
        foreground_future_covariates: Optional[TimeSeries],
    ) -> Dict[str, Dict[integer, TimeSeries]]:

        shap_values_dict = self.shap_values(
            foreground_series,
            foreground_past_covariates,
            foreground_future_covariates,
            self.n,
            self.target_names,
        )

        for t in self.target_names:
            for h in range(self.n):
                shap_values_dict[t][h] = TimeSeries.from_times_and_values(
                    shap_values_dict[t][h].time_index,
                    shap_values_dict[t][h].values,
                    columns=shap_values_dict[t][h].feature_names,
                )

        return shap_values_dict

    def shap_values(
        self,
        foreground_series: TimeSeries,
        foreground_past_covariates: Optional[TimeSeries],
        foreground_future_covariates: Optional[TimeSeries],
        horizons: Optional[Sequence[int]] = None,
        target_names: Optional[Sequence[str]] = None,
    ) -> Sequence[Sequence[shap._explanation.Explanation]]:
        """
        Return shap values Explanation objects for a given foreground TimeSeries.

        Parameters
        ----------
        foreground_series
            TimeSeries target we want to explain. Can be multivariate.
        foreground_past_covariates
            Optionally, past covariate timeseries if needed by model.
        foreground_future_covariates
            Optionally, future covariate timeseries if needed by model.
        horizons
            Optionally, a list of integer values representing which elements in the future
            we want to explain, starting from the first timestamp prediction at 0.
            For now we consider only models with output_chunk_length and it can't be bigger than
            output_chunk_length.
            If no input, then all elements of output_chunk_length will be explained.
        target_names
            Optionally, a list of string values naming the targets we want to explain.
            If no input, then all targets will be explained.

        Returns
        -------
        a shap Explanation dictionary of dictionaries of shap Explanation objects:
            - each element of the first dictionary is corresponding to a target
            - each element of the second layer dictionary is corresponding to an horizon
        """

        shap_values_dict = {}
        if target_names is None:
            target_names = self.target_names
        if horizons is None:
            horizons = range(self.n)

            for t in target_names:
                dict_t = {}
                for h in horizons:
                    dict_t[h] = self.explainers.shap_values(
                        foreground_series,
                        foreground_past_covariates,
                        foreground_future_covariates,
                        h,
                        t,
                    )
            shap_values_dict[t] = dict_t

        return shap_values_dict

    def summary_plot(
        self,
        target_names: Optional[Sequence[str]] = None,
        horizons: Optional[Sequence[int]] = None,
        nb_samples: Optional[int] = None,
        plot_type: Optional[str] = "dot",
    ):
        """
        Display a shap plot summary per target and per horizon.
        We here reuse the background data as foreground (potentially sampled) to give a general importance
        plot for each feature.
        If no target names and/or no horizons are provided, we plot all summary plots in the non specified
        dimension (target_names or horizons).

        Parameters
        ----------
        target_names
            Optionally, A list of string naming the target names we want to plot.
        horizons
            Optionally, a list of integer values representing which elements in the future
            we want to explain, starting from the first timestamp prediction at 0.
            For now we consider only models with output_chunk_length and it can't be bigger than output_chunk_length.
        nb_samples
            Optionally, an integer value sampling the foreground series (based on the backgound),
            for the sake of performance.
        plot_type
            Optionally, string value for the type of plot proposed by shap library. Currently,
            the following are available: 'dot', 'bar', 'violin'.

        """

        if target_names is not None:
            raise_if(
                any(
                    [
                        target_name not in self.target_names
                        for target_name in target_names
                    ]
                ),
                "One of the target names doesn't exist in the original background ts.",
            )

        if horizons is not None:
            # We suppose for now the output_chunk_length existence
            raise_if(
                max(horizons) > self.n - 1,
                "One of the horizons is greater than the model output_chunk_length.",
            )

        if nb_samples:
            foreground_X_sampled = shap.utils.sample(
                self.explainers.background_X, nb_samples
            )
        else:
            foreground_X_sampled = self.explainers.background_X

        shap_values = []
        if target_names is None:
            target_names = self.target_names
        if horizons is None:
            horizons = range(self.model.output_chunk_length)

        for t in target_names:
            for h in horizons:
                shap_values.append(
                    self.explainers.shap_values_from_X(foreground_X_sampled, h, t)
                )
                plt.title("Target: `{}` - Horizon: {}".format(t, "t+" + str(h)))
                shap.summary_plot(
                    shap_values[-1], foreground_X_sampled, plot_type=plot_type
                )


class _RegressionShapExplainers:
    """
    Helper Class to wrap the different cases we encounter with shap different explainers, multivariates,
    horizon etc.
    Aim to provide shap values for any type of RegressionModel. Manage the MultioutputRegressor cases.
    For darts RegressionModel only.
    TODO implement a test to not recompute each time the shap values in case of multioutput flag is True.
    """

    def __init__(
        self,
        model: GlobalForecastingModel,
        background_series: Union[TimeSeries, Sequence[TimeSeries]],
        background_past_covariates: Union[TimeSeries, Sequence[TimeSeries]],
        background_future_covariates: Union[TimeSeries, Sequence[TimeSeries]],
        n: integer,
        shap_method: Optional[ShapMethod] = None,
        background_nb_samples: Optional[int] = None,
        **kwargs,
    ):

        self.model = model
        # self.multioutput = self.model.model._get_tags()["multioutput"]
        self.is_multiOutputRegressor = isinstance(
            self.model.model, MultiOutputRegressor
        )
        self.target_dim = self.model.input_dim["target"]
        self.target_dict = {c: i for i, c in enumerate(background_series.columns)}

        self.background_X = self._create_regression_model_shap_X(
            background_series,
            background_past_covariates,
            background_future_covariates,
            background_nb_samples,
        )

        if self.is_multiOutputRegressor:
            self.explainers = {}
            for i in range(n):
                self.explainers[i] = {}
                for j in range(self.target_dim):
                    self.explainers[i][j] = self._get_explainer(
                        self.model.model.estimators_[i + j],
                        self.background_X,
                        shap_method,
                        **kwargs,
                    )
        else:
            self.explainers = self._get_explainer(
                self.model.model, self.background_X, shap_method, **kwargs
            )

        self.cache_explainers = None
        self.cache_foreground_series = None
        self.cache_foreground_past_covariates = None
        self.cache_foreground_future_covariates = None
        self.cache_foreground_X = None
        self.foreground_changed = True

    def shap_values(
        self,
        foreground_series: Union[TimeSeries, Sequence[TimeSeries]],
        foreground_past_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]],
        foreground_future_covariates: Optional[Union[TimeSeries, Sequence[TimeSeries]]],
        horizon: Optional[int] = None,
        target_name: Optional[str] = None,
    ):
        "-> shap._explanation.Explanation"

        if not all(
            foreground_series.time_index
            == foreground_past_covariates.time_index
            == foreground_future_covariates.time_index
        ):
            logger.warning(
                "The series and covariates don't share the same time index. We will take the time index common to all."
            )

        (
            foreground_series,
            foreground_past_covariates,
            foreground_future_covariates,
        ) = retain_period_common_to_all(
            [
                foreground_series,
                foreground_past_covariates,
                foreground_future_covariates,
            ]
        )

        # We don't recompute if the foreground is the same as the last computation
        if (
            (self.cache_foreground_series != foreground_series)
            or (self.cache_foreground_past_covariates != foreground_past_covariates)
            or (self.cache_foreground_future_covariates != foreground_future_covariates)
        ):

            foreground_X = self._create_regressionmodel_shap_X(
                foreground_series,
                foreground_past_covariates,
                foreground_future_covariates,
                None,
            )
            self.cache_foreground_series = foreground_series
            self.cache_foreground_past_covariates = foreground_past_covariates
            self.cache_foreground_future_covariates = foreground_future_covariates

            self.foreground_changed = True

            self.cache_foreground_X = foreground_X.copy()
        else:
            self.foreground_changed = False

        return self.shap_values_from_X(self.cache_foreground_X, horizon, target_name)

    def shap_values_from_X(
        self, X, horizon: Optional[int] = None, target_name: Optional[str] = None
    ):
        "-> shap._explanation.Explanation"

        # Multioutput case
        if self.target_dim > 1 or self.model.output_chunk_length > 1:

            # MultioutputRegressor case
            if isinstance(self.explainers, dict):
                assert isinstance(self.model.model, MultiOutputRegressor)
                shap_values = self.explainers[horizon][self.target_dict[target_name]](X)
            # Supported sikit-learn multioutput case
            else:
                # We compute it only once
                if self.foreground_changed:
                    self.cache_explainers = self.explainers(X)
                shap_values = self.cache_explainers[
                    :,
                    :,
                    horizon * self.target_dict[target_name]
                    + self.target_dict[target_name],
                ]
        else:
            shap_values = self.explainers(X)

        # We add one property to the shap._explanation.Explanation which is the index of time steps we explain
        shap_values.time_index = X.index

        # When MultiOutputRegressor, or when pure univariate and output_chun_legth = 1, we need to ravel base_values
        # to make work force plot.
        shap_values.base_values = shap_values.base_values.ravel()

        return shap_values

    def _get_explainer(
        self,
        model: GlobalForecastingModel,
        background_X: pd.DataFrame,
        shap_method: Optional[ShapMethod] = None,
        **kwargs,
    ):

        model_name = type(model).__name__
        print(model_name)
        if shap_method is None:
            shap_method = default_sklearn_shap_explainers[model_name]

        if shap_method == _ShapMethod.TREE:
            if "feature_perturbation" in kwargs:
                if kwargs.get("feature_perturbation") == "interventional":
                    explainer = shap.TreeExplainer(model, background_X, **kwargs)
                else:
                    explainer = shap.TreeExplainer(model, **kwargs)
            else:
                explainer = shap.TreeExplainer(model, **kwargs)
        elif shap_method == _ShapMethod.PERMUTATION:
            explainer = shap.PermutationExplainer(model)
        elif shap_method == _ShapMethod.KERNEL:
            print("test")
            explainer = shap.KernelExplainer(model.model.predict, background_X)
        elif shap_method == _ShapMethod.LINEAR:
            explainer = shap.LinearExplainer(model, background_X)

        logger.info("The shap method used is of type: " + str(type(explainer)))

        return explainer

    def _create_regression_model_shap_X(
        self, target_series, past_covariates, future_covariates, n_samples=None
    ):
        """
        Helper function that creates training/validation matrices (X and y as required in sklearn), given series and
        max_samples_per_ts.

        Partially adapted from _create_lagged_data funtion in regression_model

        X has the following structure:
        lags_target | lags_past_covariates | lags_future_covariates

        Where each lags_X has the following structure (lags_X=[-2,-1] and X has 2 components):
        lag_-2_comp_1_X | lag_-2_comp_2_X | lag_-1_comp_1_X | lag_-1_comp_2_X

        y has the following structure (output_chunk_length=4 and target has 2 components):
        lag_+0_comp_1_target | lag_+0_comp_2_target | ... | lag_+3_comp_1_target | lag_+3_comp_2_target
        """

        # ensure list of TimeSeries format
        if isinstance(target_series, TimeSeries):
            target_series = [target_series]
            past_covariates = [past_covariates] if past_covariates else None
            future_covariates = [future_covariates] if future_covariates else None

        Xs = []
        # iterate over series
        for idx, target_ts in enumerate(target_series):
            covariates = [
                (
                    past_covariates[idx].pd_dataframe(copy=False)
                    if past_covariates
                    else None,
                    self.model.lags.get("past"),
                ),
                (
                    future_covariates[idx].pd_dataframe(copy=False)
                    if future_covariates
                    else None,
                    self.model.lags.get("future"),
                ),
            ]

            df_X = []
            df_target = target_ts.pd_dataframe(copy=False)

            # X: target lags
            if "target" in self.model.lags:
                for lag in self.model.lags["target"]:
                    self.model.lags["target"]
                    df_tmp = df_target.shift(-lag)
                    df_X.append(
                        df_tmp.rename(
                            columns={
                                c: c + "_target_lag" + str(lag) for c in df_tmp.columns
                            }
                        )
                    )

            # X: covariate lags
            for idx, (df_cov, lags) in enumerate(covariates):
                if lags:
                    for lag in lags:
                        df_tmp = df_cov.shift(-lag)
                        if idx == 0:
                            cov_type = "past"
                        else:
                            cov_type = "fut"
                        df_X.append(
                            df_tmp.rename(
                                columns={
                                    c: c + "_" + cov_type + "_cov_lag" + str(lag)
                                    for c in df_tmp.columns
                                }
                            )
                        )

            # combine lags
            Xs.append(pd.concat(df_X, axis=1).dropna())

        # combine samples from all series
        X = pd.concat(Xs, axis=0)

        if n_samples:
            X = shap.utils.sample(n_samples, X)

        return X
